{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the basic of tensorflow and can create a model, or at least copy paste code from last notebook, then chose a optimizer,loss function and write/run the training step. We now want to study some utitites/extended libraries that extends this process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we forgot in the basic tensorflow notebook was what to do after training a model. One would like to save the model so one can load it and work with it in another program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Keras model consists of multiple components:\n",
    "\n",
    "- An architecture, or configuration, which specifyies what layers the model contain, and how they're connected.\n",
    "- A set of weights values (the \"state of the model\").\n",
    "- An optimizer (defined by compiling the model).\n",
    "- A set of losses and metrics (defined by compiling the model or calling add_loss() or add_metric()).\n",
    "\n",
    "The Keras API makes it possible to save of these pieces to disk at once, or to only selectively save some of them:\n",
    "\n",
    "- Saving everything into a single archive in the TensorFlow SavedModel format (or in the older Keras H5 format). This is the standard practice.\n",
    "- Saving the architecture / configuration only, typically as a JSON file. (Will not look into atm)\n",
    "- Saving the weights values only. This is generally used when training the model. (Will not look into atm)\n",
    "\n",
    "Let's use or model from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE SAME MODEL AS LAST NOTEBOOK\n",
    "(mnist_images, mnist_labels), _ = K.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "optimizer = K.optimizers.Adam()\n",
    "loss_object = K.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = K.Sequential([\n",
    "  K.layers.Conv2D(16,[3,3], activation='relu',\n",
    "                         input_shape=(None, None, 1)),\n",
    "  K.layers.Conv2D(16,[3,3], activation='relu'),\n",
    "  K.layers.GlobalAveragePooling2D(),\n",
    "  K.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "    \n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "    \n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      train_step(images, labels)\n",
    "    print ('Epoch {} finished'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the whole model with: (this is Tensorflow Savemodel)\n",
    "- model.save(path+'name')\n",
    "- models.load_model(path+'name')\n",
    "\n",
    "This will create a folder named name and contains 3 folders \n",
    "- assets  \n",
    "- saved_model.pb  \n",
    "- variables\n",
    "\n",
    "The model architecture, and training configuration (including the optimizer, losses, and metrics) are stored in saved_model.pb, the rest in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\jonathan\\pycharmprojects\\notebook\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\jonathan\\pycharmprojects\\notebook\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: mnist_model\\assets\n"
     ]
    }
   ],
   "source": [
    "mnist_model.save('mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is DE56-6F87\n",
      "\n",
      " Directory of C:\\Users\\Jonathan\\PycharmProjects\\ML-notebook\\Tensorflow\\mnist_model\n",
      "\n",
      "2020-08-29  20:36    <DIR>          .\n",
      "2020-08-29  20:36    <DIR>          ..\n",
      "2020-08-29  20:36    <DIR>          assets\n",
      "2020-08-29  20:36            73ÿ090 saved_model.pb\n",
      "2020-08-29  20:36    <DIR>          variables\n",
      "               1 File(s)         73ÿ090 bytes\n",
      "               4 Dir(s)  18ÿ282ÿ639ÿ360 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls mnist_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime when training a model, the data is to big to be in the memory at the same time or the process of reading/loading a sample takes longer then running the model. Tensorflow has a built in function that handles the input pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n"
     ]
    }
   ],
   "source": [
    "#Create from array, also from numpy etc.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of the input line is the dataset, we can construct a dataset from data in memory by\n",
    "- tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "This create a Python iterable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "1\n",
      "---------------------\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#iterate like always\n",
    "for elem in dataset:\n",
    "  print(elem.numpy())\n",
    "\n",
    "print('---------------------' +'\\n')\n",
    "\n",
    "it = iter(dataset)\n",
    "\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date             object\n",
      "Open            float64\n",
      "High            float64\n",
      "Low             float64\n",
      "Close           float64\n",
      "Volume            int64\n",
      "USD3MTD156N     float64\n",
      "BAMLH0A0HYM2    float64\n",
      "T10YIE          float64\n",
      "UNRATE          float64\n",
      "VIXCLS          float64\n",
      "NIKKEI225       float64\n",
      "NASDAQCOM       float64\n",
      "MS_Close        float64\n",
      "MS_Volume       float64\n",
      "JPM_Close       float64\n",
      "JPM_Volume      float64\n",
      "WFC_Close       float64\n",
      "WFC_Volume      float64\n",
      "C_Close         float64\n",
      "C_Volume        float64\n",
      "BAC_Close       float64\n",
      "BAC_Volume      float64\n",
      "BCS_Close       float64\n",
      "BCS_Volume      float64\n",
      "HSBC_Close      float64\n",
      "HSBC_Volume     float64\n",
      "ma7             float64\n",
      "ma21            float64\n",
      "26ema           float64\n",
      "12ema           float64\n",
      "MACD            float64\n",
      "20sd            float64\n",
      "upper_band      float64\n",
      "lower_band      float64\n",
      "ema             float64\n",
      "momentum        float64\n",
      "fft3            float64\n",
      "fft6            float64\n",
      "fft9            float64\n",
      "ARIMA           float64\n",
      "dtype: object\n",
      "(<tf.Tensor: shape=(), dtype=int64, numpy=0>, <tf.Tensor: shape=(40,), dtype=float64, numpy=\n",
      "array([1.43837928e+02, 1.46279804e+02, 1.43528395e+02, 1.45170639e+02,\n",
      "       6.40180000e+06,            nan,            nan,            nan,\n",
      "                  nan,            nan,            nan,            nan,\n",
      "       2.48396683e+01, 9.87690000e+06, 3.19955025e+01, 2.01431000e+07,\n",
      "       2.01559296e+01, 2.89424000e+07, 3.00267620e+01, 1.98758000e+07,\n",
      "       1.33545008e+01, 9.43226000e+07, 1.22075300e+01, 9.90000000e+05,\n",
      "       3.29889641e+01, 4.15200000e+05,            nan,            nan,\n",
      "       1.45170639e+02, 1.45170639e+02, 0.00000000e+00,            nan,\n",
      "                  nan,            nan, 1.45170639e+02,            nan,\n",
      "       1.66878635e+02, 1.67463461e+02, 1.62151119e+02, 1.66518535e+02])>)\n",
      "(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorSpec(shape=(40,), dtype=tf.float64, name=None))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('input/GS.csv')\n",
    "\n",
    "#remove object type, alt. change it to int (or string?)\n",
    "print(df.dtypes)\n",
    "del df['Date']\n",
    "\n",
    "#We can add features\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df.index,df.values))\n",
    "\n",
    "it = iter(dataset)\n",
    "print(next(it))\n",
    "\n",
    "#We see that it has shape ((),(40,))\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how to create a dataset from data from memory. To "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
